{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "from pickle import load\n", 
        "from numpy import array\n", 
        "from keras.preprocessing.text import Tokenizer\n", 
        "from keras.preprocessing.sequence import pad_sequences\n", 
        "from keras.utils.vis_utils import plot_model\n", 
        "from keras.models import Model\n", 
        "from keras.layers import Input\n", 
        "from keras.layers import Dense\n", 
        "from keras.layers import Flatten\n", 
        "from keras.layers import Dropout\n", 
        "from keras.layers import Embedding\n", 
        "from keras.layers.convolutional import Conv1D\n", 
        "from keras.layers.convolutional import MaxPooling1D\n", 
        "from keras.layers.merge import concatenate\n", 
        "from PIL import Image\n", 
        "from IPython.display import display # to display images\n", 
        "\n", 
        "# load a clean dataset\n", 
        "def load_dataset(filename):\n", 
        "\treturn load(open(filename, 'rb'))\n", 
        "\n", 
        "# fit a tokenizer\n", 
        "def create_tokenizer(lines):\n", 
        "\ttokenizer = Tokenizer()\n", 
        "\ttokenizer.fit_on_texts(lines)\n", 
        "\treturn tokenizer\n", 
        "\n", 
        "# calculate the maximum document length\n", 
        "def max_length(lines):\n", 
        "\treturn max([len(s.split()) for s in lines])\n", 
        "\n", 
        "# encode a list of lines\n", 
        "def encode_text(tokenizer, lines, length):\n", 
        "\t# integer encode\n", 
        "\tencoded = tokenizer.texts_to_sequences(lines)\n", 
        "\t# pad encoded sequences\n", 
        "\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\n", 
        "\treturn padded\n", 
        "\n", 
        "# define the model\n", 
        "def define_model(length, vocab_size):\n", 
        "\t# channel 1\n", 
        "\tinputs1 = Input(shape=(length,))\n", 
        "\tembedding1 = Embedding(vocab_size, 100)(inputs1)\n", 
        "\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n", 
        "\tdrop1 = Dropout(0.5)(conv1)\n", 
        "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\n", 
        "\tflat1 = Flatten()(pool1)\n", 
        "\t# channel 2\n", 
        "\tinputs2 = Input(shape=(length,))\n", 
        "\tembedding2 = Embedding(vocab_size, 100)(inputs2)\n", 
        "\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n", 
        "\tdrop2 = Dropout(0.5)(conv2)\n", 
        "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n", 
        "\tflat2 = Flatten()(pool2)\n", 
        "\t# channel 3\n", 
        "\tinputs3 = Input(shape=(length,))\n", 
        "\tembedding3 = Embedding(vocab_size, 100)(inputs3)\n", 
        "\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n", 
        "\tdrop3 = Dropout(0.5)(conv3)\n", 
        "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n", 
        "\tflat3 = Flatten()(pool3)\n", 
        "\t# merge\n", 
        "\tmerged = concatenate([flat1, flat2, flat3])\n", 
        "\t# interpretation\n", 
        "\tdense1 = Dense(10, activation='relu')(merged)\n", 
        "\toutputs = Dense(1, activation='sigmoid')(dense1)\n", 
        "\tmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n", 
        "\t# compile\n", 
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n", 
        "\t# summarize\n", 
        "\tmodel.summary()\n", 
        "\tplot_model(model, show_shapes=True, to_file='model.png')\n", 
        "\timage = Image.open('model.png')\n", 
        "\tdisplay(image)\n", 
        "\treturn model\n", 
        "\n", 
        "# load training dataset\n", 
        "trainLines, trainLabels = load_dataset('train.pkl')\n", 
        "# create tokenizer\n", 
        "tokenizer = create_tokenizer(trainLines)\n", 
        "# calculate max document length\n", 
        "length = max_length(trainLines)\n", 
        "print('Max document length: %d' % length)\n", 
        "# calculate vocabulary size\n", 
        "vocab_size = len(tokenizer.word_index) + 1\n", 
        "print('Vocabulary size: %d' % vocab_size)\n", 
        "# encode data\n", 
        "trainX = encode_text(tokenizer, trainLines, length)\n", 
        "# define model\n", 
        "model = define_model(length, vocab_size)\n", 
        "# fit model\n", 
        "model.fit([trainX,trainX,trainX], array(trainLabels), epochs=7, batch_size=16)\n", 
        "# save the model\n", 
        "model.save('model.h5')\n", 
        "\n"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}
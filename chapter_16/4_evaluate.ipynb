{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "from pickle import load\n", 
        "from numpy import array\n", 
        "from keras.preprocessing.text import Tokenizer\n", 
        "from keras.preprocessing.sequence import pad_sequences\n", 
        "from keras.models import load_model\n", 
        "\n", 
        "# load a clean dataset\n", 
        "def load_dataset(filename):\n", 
        "\treturn load(open(filename, 'rb'))\n", 
        "\n", 
        "# fit a tokenizer\n", 
        "def create_tokenizer(lines):\n", 
        "\ttokenizer = Tokenizer()\n", 
        "\ttokenizer.fit_on_texts(lines)\n", 
        "\treturn tokenizer\n", 
        "\n", 
        "# calculate the maximum document length\n", 
        "def max_length(lines):\n", 
        "\treturn max([len(s.split()) for s in lines])\n", 
        "\n", 
        "# encode a list of lines\n", 
        "def encode_text(tokenizer, lines, length):\n", 
        "\t# integer encode\n", 
        "\tencoded = tokenizer.texts_to_sequences(lines)\n", 
        "\t# pad encoded sequences\n", 
        "\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\n", 
        "\treturn padded\n", 
        "\n", 
        "# load datasets\n", 
        "trainLines, trainLabels = load_dataset('train.pkl')\n", 
        "testLines, testLabels = load_dataset('test.pkl')\n", 
        "# create tokenizer\n", 
        "tokenizer = create_tokenizer(trainLines)\n", 
        "# calculate max document length\n", 
        "length = max_length(trainLines)\n", 
        "print('Max document length: %d' % length)\n", 
        "# calculate vocabulary size\n", 
        "vocab_size = len(tokenizer.word_index) + 1\n", 
        "print('Vocabulary size: %d' % vocab_size)\n", 
        "# encode data\n", 
        "trainX = encode_text(tokenizer, trainLines, length)\n", 
        "testX = encode_text(tokenizer, testLines, length)\n", 
        "# load the model\n", 
        "model = load_model('model.h5')\n", 
        "# evaluate model on training dataset\n", 
        "_, acc = model.evaluate([trainX,trainX,trainX], array(trainLabels), verbose=0)\n", 
        "print('Train Accuracy: %.2f' % (acc*100))\n", 
        "# evaluate model on test dataset dataset\n", 
        "_, acc = model.evaluate([testX,testX,testX], array(testLabels), verbose=0)\n", 
        "print('Test Accuracy: %.2f' % (acc*100))"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}
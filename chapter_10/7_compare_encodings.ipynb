{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "import string\n", 
        "import re\n", 
        "from os import listdir\n", 
        "from numpy import array\n", 
        "from nltk.corpus import stopwords\n", 
        "from keras.preprocessing.text import Tokenizer\n", 
        "from keras.models import Sequential\n", 
        "from keras.layers import Dense\n", 
        "from pandas import DataFrame\n", 
        "%matplotlib inline\n", 
        "from matplotlib import pyplot\n", 
        "\n", 
        "# load doc into memory\n", 
        "def load_doc(filename):\n", 
        "\t# open the file as read only\n", 
        "\tfile = open(filename, 'r')\n", 
        "\t# read all text\n", 
        "\ttext = file.read()\n", 
        "\t# close the file\n", 
        "\tfile.close()\n", 
        "\treturn text\n", 
        "\n", 
        "# turn a doc into clean tokens\n", 
        "def clean_doc(doc):\n", 
        "\t# split into tokens by white space\n", 
        "\ttokens = doc.split()\n", 
        "\t# prepare regex for char filtering\n", 
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n", 
        "\t# remove punctuation from each word\n", 
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n", 
        "\t# remove remaining tokens that are not alphabetic\n", 
        "\ttokens = [word for word in tokens if word.isalpha()]\n", 
        "\t# filter out stop words\n", 
        "\tstop_words = set(stopwords.words('english'))\n", 
        "\ttokens = [w for w in tokens if not w in stop_words]\n", 
        "\t# filter out short tokens\n", 
        "\ttokens = [word for word in tokens if len(word) > 1]\n", 
        "\treturn tokens\n", 
        "\n", 
        "# load doc, clean and return line of tokens\n", 
        "def doc_to_line(filename, vocab):\n", 
        "\t# load the doc\n", 
        "\tdoc = load_doc(filename)\n", 
        "\t# clean doc\n", 
        "\ttokens = clean_doc(doc)\n", 
        "\t# filter by vocab\n", 
        "\ttokens = [w for w in tokens if w in vocab]\n", 
        "\treturn ' '.join(tokens)\n", 
        "\n", 
        "# load all docs in a directory\n", 
        "def process_docs(directory, vocab, is_train):\n", 
        "\tlines = list()\n", 
        "\t# walk through all files in the folder\n", 
        "\tfor filename in listdir(directory):\n", 
        "\t\t# skip any reviews in the test set\n", 
        "\t\tif is_train and filename.startswith('cv9'):\n", 
        "\t\t\tcontinue\n", 
        "\t\tif not is_train and not filename.startswith('cv9'):\n", 
        "\t\t\tcontinue\n", 
        "\t\t# create the full path of the file to open\n", 
        "\t\tpath = directory + '/' + filename\n", 
        "\t\t# load and clean the doc\n", 
        "\t\tline = doc_to_line(path, vocab)\n", 
        "\t\t# add to list\n", 
        "\t\tlines.append(line)\n", 
        "\treturn lines\n", 
        "\n", 
        "# load and clean a dataset\n", 
        "def load_clean_dataset(vocab, is_train):\n", 
        "\t# load documents\n", 
        "\tneg = process_docs('txt_sentoken/neg', vocab, is_train)\n", 
        "\tpos = process_docs('txt_sentoken/pos', vocab, is_train)\n", 
        "\tdocs = neg + pos\n", 
        "\t# prepare labels\n", 
        "\tlabels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n", 
        "\treturn docs, labels\n", 
        "\n", 
        "# define the model\n", 
        "def define_model(n_words):\n", 
        "\t# define network\n", 
        "\tmodel = Sequential()\n", 
        "\tmodel.add(Dense(50, input_shape=(n_words,), activation='relu'))\n", 
        "\tmodel.add(Dense(1, activation='sigmoid'))\n", 
        "\t# compile network\n", 
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n", 
        "\treturn model\n", 
        "\n", 
        "# evaluate a neural network model\n", 
        "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n", 
        "\tscores = list()\n", 
        "\tn_repeats = 10\n", 
        "\tn_words = Xtest.shape[1]\n", 
        "\tfor i in range(n_repeats):\n", 
        "\t\t# define network\n", 
        "\t\tmodel = define_model(n_words)\n", 
        "\t\t# fit network\n", 
        "\t\tmodel.fit(Xtrain, ytrain, epochs=10, verbose=0)\n", 
        "\t\t# evaluate\n", 
        "\t\t_, acc = model.evaluate(Xtest, ytest, verbose=0)\n", 
        "\t\tscores.append(acc)\n", 
        "\t\tprint('%d accuracy: %s' % ((i+1), acc))\n", 
        "\treturn scores\n", 
        "\n", 
        "# prepare bag of words encoding of docs\n", 
        "def prepare_data(train_docs, test_docs, mode):\n", 
        "\t# create the tokenizer\n", 
        "\ttokenizer = Tokenizer()\n", 
        "\t# fit the tokenizer on the documents\n", 
        "\ttokenizer.fit_on_texts(train_docs)\n", 
        "\t# encode training data set\n", 
        "\tXtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n", 
        "\t# encode training data set\n", 
        "\tXtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n", 
        "\treturn Xtrain, Xtest\n", 
        "\n", 
        "# load the vocabulary\n", 
        "vocab_filename = 'vocab.txt'\n", 
        "vocab = load_doc(vocab_filename)\n", 
        "vocab = set(vocab.split())\n", 
        "# load all reviews\n", 
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n", 
        "test_docs, ytest = load_clean_dataset(vocab, False)\n", 
        "# run experiment\n", 
        "modes = ['binary', 'count', 'tfidf', 'freq']\n", 
        "results = DataFrame()\n", 
        "for mode in modes:\n", 
        "\t# prepare data for mode\n", 
        "\tXtrain, Xtest = prepare_data(train_docs, test_docs, mode)\n", 
        "\t# evaluate model on data for mode\n", 
        "\tresults[mode] = evaluate_mode(Xtrain, ytrain, Xtest, ytest)\n", 
        "# summarize results\n", 
        "print(results.describe())\n", 
        "# plot results\n", 
        "results.boxplot()\n", 
        "pyplot.show()\n"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}
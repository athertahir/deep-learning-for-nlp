{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "from random import randint\n", 
        "from pickle import load\n", 
        "from keras.models import load_model\n", 
        "from keras.preprocessing.sequence import pad_sequences\n", 
        "\n", 
        "# load doc into memory\n", 
        "def load_doc(filename):\n", 
        "\t# open the file as read only\n", 
        "\tfile = open(filename, 'r')\n", 
        "\t# read all text\n", 
        "\ttext = file.read()\n", 
        "\t# close the file\n", 
        "\tfile.close()\n", 
        "\treturn text\n", 
        "\n", 
        "# generate a sequence from a language model\n", 
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n", 
        "\tresult = list()\n", 
        "\tin_text = seed_text\n", 
        "\t# generate a fixed number of words\n", 
        "\tfor _ in range(n_words):\n", 
        "\t\t# encode the text as integer\n", 
        "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n", 
        "\t\t# truncate sequences to a fixed length\n", 
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n", 
        "\t\t# predict probabilities for each word\n", 
        "\t\tyhat = model.predict_classes(encoded, verbose=0)\n", 
        "\t\t# map predicted word index to word\n", 
        "\t\tout_word = ''\n", 
        "\t\tfor word, index in tokenizer.word_index.items():\n", 
        "\t\t\tif index == yhat:\n", 
        "\t\t\t\tout_word = word\n", 
        "\t\t\t\tbreak\n", 
        "\t\t# append to input\n", 
        "\t\tin_text += ' ' + out_word\n", 
        "\t\tresult.append(out_word)\n", 
        "\treturn ' '.join(result)\n", 
        "\n", 
        "# load cleaned text sequences\n", 
        "in_filename = 'republic_sequences.txt'\n", 
        "doc = load_doc(in_filename)\n", 
        "lines = doc.split('\\n')\n", 
        "seq_length = len(lines[0].split()) - 1\n", 
        "# load the model\n", 
        "model = load_model('model.h5')\n", 
        "# load the tokenizer\n", 
        "tokenizer = load(open('tokenizer.pkl', 'rb'))\n", 
        "# select a seed text\n", 
        "seed_text = lines[randint(0,len(lines))]\n", 
        "print(seed_text + '\\n')\n", 
        "# generate new text\n", 
        "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n", 
        "print(generated)"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}
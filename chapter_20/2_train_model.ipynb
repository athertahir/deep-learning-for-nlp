{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "from numpy import array\n", 
        "from pickle import dump\n", 
        "from keras.preprocessing.text import Tokenizer\n", 
        "from keras.utils.vis_utils import plot_model\n", 
        "from keras.utils import to_categorical\n", 
        "from keras.models import Sequential\n", 
        "from keras.layers import Dense\n", 
        "from keras.layers import LSTM\n", 
        "from keras.layers import Embedding\n", 
        "from PIL import Image\n", 
        "from IPython.display import display # to display images\n", 
        "\n", 
        "# load doc into memory\n", 
        "def load_doc(filename):\n", 
        "\t# open the file as read only\n", 
        "\tfile = open(filename, 'r')\n", 
        "\t# read all text\n", 
        "\ttext = file.read()\n", 
        "\t# close the file\n", 
        "\tfile.close()\n", 
        "\treturn text\n", 
        "\n", 
        "# define the model\n", 
        "def define_model(vocab_size, seq_length):\n", 
        "\tmodel = Sequential()\n", 
        "\tmodel.add(Embedding(vocab_size, 50, input_length=seq_length))\n", 
        "\tmodel.add(LSTM(100, return_sequences=True))\n", 
        "\tmodel.add(LSTM(100))\n", 
        "\tmodel.add(Dense(100, activation='relu'))\n", 
        "\tmodel.add(Dense(vocab_size, activation='softmax'))\n", 
        "\t# compile network\n", 
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n", 
        "\t# summarize defined model\n", 
        "\tmodel.summary()\n", 
        "\tplot_model(model, to_file='model.png', show_shapes=True)\n", 
        "\timage = Image.open('model.png')\n", 
        "\tdisplay(image)\n", 
        "\treturn model\n", 
        "\n", 
        "# load\n", 
        "in_filename = 'republic_sequences.txt'\n", 
        "doc = load_doc(in_filename)\n", 
        "lines = doc.split('\\n')\n", 
        "# integer encode sequences of words\n", 
        "tokenizer = Tokenizer()\n", 
        "tokenizer.fit_on_texts(lines)\n", 
        "sequences = tokenizer.texts_to_sequences(lines)\n", 
        "# vocabulary size\n", 
        "vocab_size = len(tokenizer.word_index) + 1\n", 
        "# separate into input and output\n", 
        "sequences = array(sequences)\n", 
        "X, y = sequences[:,:-1], sequences[:,-1]\n", 
        "y = to_categorical(y, num_classes=vocab_size)\n", 
        "seq_length = X.shape[1]\n", 
        "# define model\n", 
        "model = define_model(vocab_size, seq_length)\n", 
        "# fit model\n", 
        "model.fit(X, y, batch_size=128, epochs=100)\n", 
        "# save the model to file\n", 
        "model.save('model.h5')\n", 
        "# save the tokenizer\n", 
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}